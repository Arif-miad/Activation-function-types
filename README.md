# Activation-function-types
Sigmoid Function: This function maps any input value to a range between 0 and 1 ReLU (Rectified Linear Unit): ReLU function outputs the input directly if it is positive, otherwise, it outputs zero Tanh Function: Similar to the sigmoid function, but maps input values to a range between -1 and 1 Softmax Function: 
